{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregúntale a la Corte Constitucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Título de la imagen](img/Architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_community.document_loaders.mongodb import MongodbLoader\n",
    "import nest_asyncio\n",
    "from pymongo.mongo_client import MongoClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_nomic import NomicEmbeddings\n",
    "import pymongo\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"USER_AGENT\"] = \"MyApp/1.0\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv('GROQ_API_KEY')\n",
    "client = pymongo.MongoClient(os.environ['MONGODB_URI'])\n",
    "collections = client.get_database(os.environ['MONGODB_DB']).get_collection(os.environ['MONGODB_COLLECTION'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función para realizar scraping y guardar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scraping_sentencias(termino_de_busqueda):\n",
    "    # Construir la URL para la búsqueda\n",
    "    termino_de_busqueda = termino_de_busqueda.replace(' ', '+')\n",
    "    URL = 'https://www.corteconstitucional.gov.co/relatoria/buscador_new/?searchOption=texto&fini=1992-01-01&ffin=2024-10-29&buscar_por='+ termino_de_busqueda +'&accion=search&verform=si&slop=1&volver_a=relatoria&qu=625&maxprov=100&OrderbyOption=des__score'\n",
    "\n",
    "    # Realizar la solicitud GET a la página\n",
    "    response = requests.get(URL)\n",
    "\n",
    "    # Verificar si la solicitud fue exitosa\n",
    "    if response.status_code == 200:\n",
    "        # Parsear el contenido HTML con BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Encontrar todas las etiquetas 'a' con atributo 'href'\n",
    "        enlaces = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "    else:\n",
    "        print(f\"Error al acceder a la página: {response.status_code}\")\n",
    "\n",
    "    # crear una lista con los enlaces obtenidos anteriormente\n",
    "\n",
    "    lista_enlaces = []\n",
    "\n",
    "    # Encontrar todas las etiquetas 'a' con atributo 'href'\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        lista_enlaces.append(a['href'])\n",
    "\n",
    "\n",
    "    # filtrar lista_enlaces y coger solo los enlaces contengan la palabra relatoria\n",
    "\n",
    "    enlaces_relatoria = [enlace for enlace in lista_enlaces if 'relatoria' in enlace]\n",
    "    enlaces_relatoria = [enlace for enlace in enlaces_relatoria if len(enlace) > 49]\n",
    "\n",
    "    # Imprimir la lista de enlaces filtrados\n",
    "    enlaces_relatoria\n",
    "\n",
    "    # diccionario_relatorias\n",
    "    diccionario_relatorias = {}\n",
    "\n",
    "    for enlace in enlaces_relatoria:\n",
    "        try:\n",
    "            nota = requests.get(enlace)\n",
    "            nota.raise_for_status()  # Verifica si hubo algún problema con la respuesta HTTP\n",
    "            s_nota = BeautifulSoup(nota.text, 'html.parser')\n",
    "            texto = (s_nota.find('div', attrs={'class': 'WordSection1'}).text).strip()\n",
    "            diccionario_relatorias[enlace] = texto\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error al solicitar el enlace {enlace}: {e}\")\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                # Si no encuentra 'WordSection1', intenta con 'Section1'\n",
    "                texto = (s_nota.find('div', attrs={'class': 'Section1'}).text).strip()\n",
    "                diccionario_relatorias[enlace] = texto\n",
    "            except AttributeError as e:\n",
    "                print(f\"Error procesando el contenido del enlace {enlace}: {e}\")\n",
    "\n",
    "    diccionario_relatorias\n",
    "\n",
    "    # convertir el diccionario en un df de pandas\n",
    "\n",
    "    df = pd.DataFrame(list(diccionario_relatorias.items()), columns=['Enlace', 'Texto'])\n",
    "\n",
    "    df['Sentencia'] = df['Enlace'].str.split('/relatoria/').str[-1].str.split('.htm').str[0]\n",
    "\n",
    "    # Reorganizar el DataFrame\n",
    "    df = df[['Sentencia', 'Texto']]  # Selecciona las columnas en el orden deseado\n",
    "\n",
    "    # exportar el df en formato JSON Lines\n",
    "\n",
    "    nombre_json = ('sentencias_' + termino_de_busqueda).replace('+', '_') + '.jsonl'\n",
    "    # Assuming df is your DataFrame\n",
    "    df.to_json(nombre_json, orient='records', lines=True)\n",
    "\n",
    "    # Leer el archivo JSON Lines y cargar los documentos\n",
    "    with open(nombre_json, 'r') as f:\n",
    "        docs_to_insert = [json.loads(line) for line in f]\n",
    "\n",
    "    # Splitting \n",
    "\n",
    "    from langchain.text_splitter import CharacterTextSplitter\n",
    "    from langchain.docstore.document import Document\n",
    "\n",
    "    docs_to_insert = [\n",
    "        Document(page_content=doc['Texto'], metadata={'sentencia': doc['Sentencia']})\n",
    "        for doc in docs_to_insert\n",
    "    ]\n",
    "\n",
    "    def custom_split(text, max_size=1000):\n",
    "        chunks = []\n",
    "        while len(text) > max_size:\n",
    "            chunk = text[:max_size]\n",
    "            chunks.append(chunk)\n",
    "            text = text[max_size:]\n",
    "        if text:\n",
    "            chunks.append(text)\n",
    "        return chunks\n",
    "\n",
    "    docs_splits = []\n",
    "    for doc in docs_to_insert:\n",
    "        chunks = custom_split(doc.page_content)\n",
    "        for chunk in chunks:\n",
    "            docs_splits.append(Document(page_content=chunk, metadata=doc.metadata))\n",
    "    docs_splits_dict = [doc.dict() for doc in docs_splits]\n",
    "\n",
    "    collections.insert_many(docs_splits_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de la conexión con MongoDB Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def configurar_modelo():\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "    load_dotenv()\n",
    "\n",
    "    if collections.count_documents({}) == 0:\n",
    "        raise ValueError(\"No hay documentos en la base de datos. Por favor, realice una búsqueda primero.\")\n",
    "\n",
    "    loader = MongodbLoader(\n",
    "        connection_string=os.environ['MONGODB_URI'],\n",
    "        db_name=os.environ['MONGODB_DB'],\n",
    "        collection_name=os.environ['MONGODB_COLLECTION'],\n",
    "        filter_criteria={},\n",
    "        field_names=[\"metadata\", \"page_content\"],\n",
    "    )\n",
    "    docs = loader.load()\n",
    "\n",
    "    if not docs:\n",
    "        raise ValueError(\"No se encontraron documentos en la base de datos.\")\n",
    "\n",
    "    # Insertar documentos en la base de datos\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "    vectorStore = MongoDBAtlasVectorSearch.from_documents( \n",
    "        documents= docs,\n",
    "        embedding= embeddings, \n",
    "        collection= collections,\n",
    "        index_name=os.environ['MONGODB_VECTOR_INDEX']\n",
    "    )\n",
    "\n",
    "    retriever = vectorStore.as_retriever(search_kwargs={\"similarity_threshold\": 0.1})\n",
    "\n",
    "\n",
    "    template = \"\"\"Quiero un análisis jurídico profesional sobre los derechos fundamentales amenazados o dañados que se debaten en la Corte Constitucional de Colombia. Es importante conocer los hechos de acuerdo a las circunstancias de modo, el tiempo con las fechas y hora, el lugar\n",
    "    donde ocurrieron y las personas naturales o jurídicas que tienen conflicto entre ellas. También es importante conocer cuál fue el daño o peligro que afecta los derechos fundamentales dentro de las consideraciones tenidas en cuenta por la Corte Constitucional. Finalmente, requiero saber lo que resuelve la Corte Constitucional. Con base en las anteriores instrucciones, proporciona un resumen de:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    #model = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "    model = ChatGroq(temperature=0, model=\"llama-3.1-70b-versatile\")\n",
    "    # Local LLM\n",
    "    ollama_llm = \"phi3.5\"\n",
    "    model_local = ChatOllama(model=ollama_llm)\n",
    "\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "chain = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_chain():\n",
    "    try:\n",
    "        chain = configurar_modelo()\n",
    "        return chain\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error al iniciar el chain: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
